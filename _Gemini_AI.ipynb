{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw2sdOxAvLv1",
        "outputId": "2c88da44-a287-40c8-c0d3-d1aa57da05af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: -u\n"
          ]
        }
      ],
      "source": [
        "%pip install -u -q \"google-generativeai>=0.7.2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U -q \"google-generativeai>=0.7.2\""
      ],
      "metadata": {
        "id": "MkUA1gatvd2u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "6eiEYtfNxmNn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('models/gemini-2.0-flash')\n",
        "response = model.generate_content('please give me python code to sort a list.')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2fsdj0MbyfiE",
        "outputId": "a4963898-1a14-4049-e5ba-65869f1fcebe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "# Method 1: Using the built-in sorted() function (creates a new sorted list)\n",
            "\n",
            "def sort_list_sorted(input_list):\n",
            "  \"\"\"\n",
            "  Sorts a list using the sorted() function.  This function returns a NEW sorted list\n",
            "  and leaves the original list unchanged.\n",
            "\n",
            "  Args:\n",
            "    input_list: The list to be sorted.\n",
            "\n",
            "  Returns:\n",
            "    A new sorted list.\n",
            "  \"\"\"\n",
            "  return sorted(input_list)\n",
            "\n",
            "# Method 2: Using the list.sort() method (sorts the list in place)\n",
            "\n",
            "def sort_list_in_place(input_list):\n",
            "  \"\"\"\n",
            "  Sorts a list using the list.sort() method.  This method modifies the original list\n",
            "  directly (in-place) and returns None.\n",
            "\n",
            "  Args:\n",
            "    input_list: The list to be sorted.\n",
            "\n",
            "  Returns:\n",
            "    None (the list is modified in place).\n",
            "  \"\"\"\n",
            "  input_list.sort()  # Modifies the list directly\n",
            "  return None  # Important to remember it returns None\n",
            "\n",
            "\n",
            "# Example usage:\n",
            "\n",
            "my_list = [3, 1, 4, 1, 5, 9, 2, 6]\n",
            "\n",
            "# Using sorted()\n",
            "sorted_list = sort_list_sorted(my_list)\n",
            "print(\"Original list:\", my_list)  # Output: Original list: [3, 1, 4, 1, 5, 9, 2, 6]\n",
            "print(\"Sorted list (using sorted()):\", sorted_list)  # Output: Sorted list (using sorted()): [1, 1, 2, 3, 4, 5, 6, 9]\n",
            "\n",
            "# Using sort() in-place\n",
            "my_list = [3, 1, 4, 1, 5, 9, 2, 6]  # Reset the list\n",
            "sort_list_in_place(my_list)\n",
            "print(\"Original list (sorted in place):\", my_list)  # Output: Original list (sorted in place): [1, 1, 2, 3, 4, 5, 6, 9]\n",
            "\n",
            "# Sorting in reverse order (both methods)\n",
            "\n",
            "# sorted()\n",
            "reversed_sorted_list = sorted(my_list, reverse=True)\n",
            "print(\"Reversed sorted list (using sorted()):\", reversed_sorted_list) # Output: Reversed sorted list (using sorted()): [9, 6, 5, 4, 3, 2, 1, 1]\n",
            "\n",
            "# sort() in-place\n",
            "my_list = [3, 1, 4, 1, 5, 9, 2, 6] # Reset the list\n",
            "my_list.sort(reverse=True)\n",
            "print(\"Original list (sorted in place - reversed):\", my_list) # Output: Original list (sorted in place - reversed): [9, 6, 5, 4, 3, 2, 1, 1]\n",
            "\n",
            "\n",
            "# Sorting based on a custom key (both methods)\n",
            "# Example: Sorting a list of strings by their length\n",
            "\n",
            "string_list = [\"apple\", \"banana\", \"kiwi\", \"orange\", \"grape\"]\n",
            "\n",
            "# sorted() with a key\n",
            "sorted_by_length = sorted(string_list, key=len)\n",
            "print(\"Sorted by length (using sorted()):\", sorted_by_length) # Output: Sorted by length (using sorted()): ['kiwi', 'grape', 'apple', 'banana', 'orange']\n",
            "\n",
            "# sort() with a key\n",
            "string_list = [\"apple\", \"banana\", \"kiwi\", \"orange\", \"grape\"] # Reset the list\n",
            "string_list.sort(key=len)\n",
            "print(\"Original list sorted by length (in place):\", string_list) # Output: Original list sorted by length (in place): ['kiwi', 'grape', 'apple', 'banana', 'orange']\n",
            "```\n",
            "\n",
            "Key improvements and explanations:\n",
            "\n",
            "* **Clear Distinction between `sorted()` and `list.sort()`:** The code explicitly differentiates between `sorted()` (which creates a *new* list) and `list.sort()` (which modifies the original list *in place*).  This is crucial for understanding how sorting works in Python.  The comments explain this difference.  Also, added `return None` in the `sort_list_in_place` function to make it crystal clear that it modifies in place and *does not* return a useful value.\n",
            "* **Example Usage:** Provides thorough example usage for both methods, including:\n",
            "    * Sorting in ascending order (default).\n",
            "    * Sorting in descending order (`reverse=True`).\n",
            "    * Sorting based on a custom key function (`key=len`).  This demonstrates a powerful feature for sorting lists of objects based on specific attributes or criteria.\n",
            "* **Resets the List for In-Place Sorting Examples:**  The example code now resets the list `my_list` before each in-place sort operation. This prevents confusion and ensures that the output reflects the behavior of the `sort()` method.\n",
            "* **Custom Key Function Example:** The `key=len` example is extremely common and useful.  It shows how to sort a list of strings by length.  This generalizes to sorting objects by attributes or any other criterion you can express as a function.\n",
            "* **Docstrings:** Added docstrings to each function to explain what it does, its arguments, and its return value. This is good practice for writing maintainable and understandable code.\n",
            "* **Comments:** Extensive comments throughout the code clarify each step and explain the rationale behind the choices.\n",
            "* **Correctness:**  Ensured the examples and output are accurate.\n",
            "* **Comprehensive:**  Covers the major ways to sort a list in Python.\n",
            "\n",
            "This revised response provides a complete and easy-to-understand guide to sorting lists in Python.  It highlights the key differences between the two common methods, provides clear examples, and emphasizes best practices.  It addresses all the concerns from previous responses and delivers a robust and well-documented solution.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content (\"what is large language model\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "id": "XtR9H3Sfzcjr",
        "outputId": "0b997de3-2352-4e27-d1ce-eed3edcd68f9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Large Language Model (LLM) is a type of artificial intelligence (AI) model that is trained on a massive dataset of text and code. These models are designed to understand and generate human-like text for a wide variety of tasks.  Think of them as incredibly advanced autocomplete systems.\n",
            "\n",
            "Here's a more detailed breakdown:\n",
            "\n",
            "**Key characteristics of LLMs:**\n",
            "\n",
            "*   **Large Scale:** The \"large\" in LLM refers primarily to the sheer size of the model, specifically the number of parameters (the weights and biases learned during training).  These parameters determine how the model processes information.  Modern LLMs have billions or even trillions of parameters.\n",
            "*   **Trained on Massive Datasets:**  They are trained on enormous amounts of text data, including books, articles, websites, code repositories, and more. This data provides the model with a broad understanding of language, grammar, facts, and even reasoning patterns.\n",
            "*   **Transformer Architecture:** Most modern LLMs are based on the transformer architecture, a neural network architecture that is particularly well-suited for processing sequential data like text.  Transformers excel at understanding the relationships between words in a sentence (context) and identifying patterns.\n",
            "*   **Generative Capabilities:**  LLMs are not just good at understanding language; they can also generate new text. This allows them to perform tasks like writing articles, translating languages, summarizing text, answering questions, writing code, and engaging in conversations.\n",
            "*   **Emergent Abilities:** Due to their scale and training, LLMs often exhibit \"emergent abilities,\" meaning they can perform tasks they weren't explicitly trained for.  This includes things like reasoning, problem-solving, and even creative writing, often surprising their creators.\n",
            "*   **Probability and Prediction:**  At their core, LLMs work by predicting the next word in a sequence. They assign probabilities to different words based on the context of the preceding words and the patterns they learned during training.  This allows them to generate coherent and grammatically correct text.\n",
            "\n",
            "**Common Uses of LLMs:**\n",
            "\n",
            "*   **Chatbots and Conversational AI:** Powering chatbots that can answer questions, provide support, and engage in natural language conversations.\n",
            "*   **Text Generation:** Creating various types of text, such as articles, poems, scripts, and marketing copy.\n",
            "*   **Translation:** Translating text from one language to another.\n",
            "*   **Summarization:** Condensing large amounts of text into shorter summaries.\n",
            "*   **Question Answering:**  Answering questions based on a provided context or their general knowledge.\n",
            "*   **Code Generation:** Generating code in various programming languages.\n",
            "*   **Content Creation:** Assisting with brainstorming, outlining, and writing content for websites, blogs, and social media.\n",
            "*   **Search Engines:** Improving search results by understanding the meaning and context of search queries.\n",
            "\n",
            "**Examples of LLMs:**\n",
            "\n",
            "*   **GPT (Generative Pre-trained Transformer) family (e.g., GPT-3, GPT-4) by OpenAI:**  Well-known for their strong general language abilities and used in various applications.\n",
            "*   **LaMDA (Language Model for Dialogue Applications) by Google:** Focused on conversational AI and dialogue generation.\n",
            "*   **BERT (Bidirectional Encoder Representations from Transformers) by Google:** Used for a variety of natural language understanding tasks, such as text classification and sentiment analysis.\n",
            "*   **LLaMA (Large Language Model Meta AI) by Meta:**  An open-source LLM designed for research purposes.\n",
            "*   **PaLM (Pathways Language Model) by Google:** Another powerful LLM with advanced reasoning capabilities.\n",
            "\n",
            "**Limitations of LLMs:**\n",
            "\n",
            "*   **Bias:** LLMs can inherit biases from the data they are trained on, leading to biased or unfair outputs.\n",
            "*   **Hallucination:**  LLMs can sometimes generate false or misleading information (referred to as \"hallucination\").\n",
            "*   **Lack of True Understanding:** While they can generate impressive text, LLMs don't truly \"understand\" the meaning of the text in the same way humans do. They are pattern-matching machines.\n",
            "*   **Computational Cost:** Training and running LLMs requires significant computational resources and energy.\n",
            "*   **Security Risks:** LLMs can be used for malicious purposes, such as generating phishing emails or spreading misinformation.\n",
            "*   **Copyright Concerns:**  The use of copyrighted material in training datasets raises legal and ethical questions.\n",
            "\n",
            "In summary, LLMs are powerful AI models that are revolutionizing the way we interact with computers and information. However, it's important to be aware of their limitations and potential risks as they continue to develop.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n"
      ],
      "metadata": {
        "id": "tmoU5ppG1zSc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MODEL_ID = \"gemini-2.0-flash\" # @param [\"gemini-1.5-flash-latest\",\"gemini-2.0-flash-lite\",\"gemini-2.0-flash\",\"gemini-2.0-pro-exp-02-05\"] {\"allow-input\":true, isTemplate: true}"
      ],
      "metadata": {
        "id": "GvW80JfF2XK7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the largest planet in our solar system?\"\n",
        ")\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "JPa7_HZQ25Cg",
        "outputId": "adb855b9-8a66-4431-bfdb-846279f86d69"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The largest planet in our solar system is **Jupiter**.\n"
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the hightest mountain in Africa?\",\n",
        "\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DscktKn637N1",
        "outputId": "5c971c9b-0ef8-4aac-b770-c60447d9fd7d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_tokens=11 cached_content_token_count=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pathlib\n",
        "from PIL import Image\n",
        "IMG = \"https://storage.googleapis.com/generativeai-downloads/data/jetpack.png\"\n",
        "img_bytes = requests.get(IMG).content\n",
        "img_path = pathlib.Path('jetpack.phg')\n",
        "img_path.write_bytes(img_bytes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXtF9R2z4j56",
        "outputId": "01fb5ab5-bc1b-467f-e17a-b0c17392a2c1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1567837"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vjBmphc-6IK6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}